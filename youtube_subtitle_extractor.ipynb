{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO4Pr0+269OChIVRb1syDqf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/getcher123/YouTube-Subtitle-Extractor/blob/main/youtube_subtitle_extractor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# YouTube Subtitle Extractor\n",
        "\n",
        "This script extracts subtitles of YouTube videos in English and Russian languages, cleans the text and saves them into an Excel file for parallel corpus creation.\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "The following packages are required to run the script:\n",
        "- pandas\n",
        "- re\n",
        "- os\n",
        "- itertools\n",
        "# - YouTubeTranscriptApi (Install using `!pip install youtube_transcript_api`)\n",
        "\n",
        "## Usage\n",
        "\n",
        "1. Set the `video_ids` and `channelId` variables to the list of video IDs and YouTube channel ID for which you want to extract subtitles.\n",
        "2. Run the script in your preferred Python environment.\n",
        "\n",
        "The script will extract subtitles for the specified video IDs, clean the text, compare English and Russian subtitles to remove any discrepancies, and save the final result in an Excel file in the specified directory.\n",
        "\n",
        "The saved Excel file will have two columns:\n",
        "- `en` - English subtitle\n",
        "- `ru` - Russian subtitle\n",
        "\n",
        "## Notes\n",
        "\n",
        "- If a video doesn't have English or Russian subtitles, it will be skipped.\n",
        "- The script splits subtitles by sentence boundaries and cleans the text by removing unnecessary characters such as `..., “, ’, etc`.\n",
        "- To remove discrepancies between English and Russian subtitles, the script compares the timestamps in the subtitles and deletes the sentence that doesn't have a matching timestamp in the other language. If there are multiple discrepancies, it may leave some of them unpaired."
      ],
      "metadata": {
        "id": "T-olChaWcB-W"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "juMUUGEKA2Ey",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0cf3edda-bf33-4d2e-af68-8ed9588c0d52"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['snt_UCS1mEytYPPiOHtfe_zqvKWg_E21kilDE8jY.xlsx',\n",
              " 'snt_UCS1mEytYPPiOHtfe_zqvKWg_6NK70E9WfY0.xlsx',\n",
              " 'snt_UCS1mEytYPPiOHtfe_zqvKWg_TZIKanZqUtk.xlsx',\n",
              " 'snt_UCS1mEytYPPiOHtfe_zqvKWg_k7aGLisVvOA.xlsx',\n",
              " 'snt_UCS1mEytYPPiOHtfe_zqvKWg_LyPnYuawOJY.xlsx',\n",
              " 'snt_UCS1mEytYPPiOHtfe_zqvKWg_AxV0amhuE4s.xlsx',\n",
              " 'snt_UCS1mEytYPPiOHtfe_zqvKWg_K2lMMkKk1Hg.xlsx',\n",
              " 'snt_UCS1mEytYPPiOHtfe_zqvKWg_gkftlwflhss.xlsx',\n",
              " 'snt_UCS1mEytYPPiOHtfe_zqvKWg_6ttPvTHia_Y.xlsx',\n",
              " 'snt_UCS1mEytYPPiOHtfe_zqvKWg_lIqdaP4HSB8.xlsx',\n",
              " 'snt_UCS1mEytYPPiOHtfe_zqvKWg_QgnIVcTvjfg.xlsx',\n",
              " 'snt_UCS1mEytYPPiOHtfe_zqvKWg_qiJfTa4C6HI.xlsx',\n",
              " 'snt_UCS1mEytYPPiOHtfe_zqvKWg_VYBYgIvFfCI.xlsx',\n",
              " 'snt_UCS1mEytYPPiOHtfe_zqvKWg_NfMxbPQKr3g.xlsx',\n",
              " 'snt_UCS1mEytYPPiOHtfe_zqvKWg_y1497ksjLWk.xlsx',\n",
              " 'snt_UCS1mEytYPPiOHtfe_zqvKWg_e6AaXosgiYM.xlsx',\n",
              " 'snt_UCS1mEytYPPiOHtfe_zqvKWg_-UgFOrP8D8Q.xlsx',\n",
              " 'snt_UCS1mEytYPPiOHtfe_zqvKWg_40isgJFwMGg.xlsx',\n",
              " 'snt_UCS1mEytYPPiOHtfe_zqvKWg_Vb7dKjAOOQ0.xlsx',\n",
              " 'snt_UCS1mEytYPPiOHtfe_zqvKWg_8b_xmMGCiPg.xlsx',\n",
              " 'snt_UCS1mEytYPPiOHtfe_zqvKWg_DStFRiEyd_8.xlsx',\n",
              " 'snt_UCS1mEytYPPiOHtfe_zqvKWg_0Xk2Npmn-GQ.xlsx',\n",
              " 'snt_UCS1mEytYPPiOHtfe_zqvKWg_PBetrhjWHJM.xlsx',\n",
              " 'snt_UCS1mEytYPPiOHtfe_zqvKWg_sw81zDLc3yY.xlsx',\n",
              " 'snt_UCS1mEytYPPiOHtfe_zqvKWg_KXxQl5zITkw.xlsx',\n",
              " 'snt_UCS1mEytYPPiOHtfe_zqvKWg_BQEUMUcp4aE.xlsx',\n",
              " 'snt_UCS1mEytYPPiOHtfe_zqvKWg_4x54-UAJp1Y.xlsx',\n",
              " 'snt_UCS1mEytYPPiOHtfe_zqvKWg_8_ErdFkTsag.xlsx',\n",
              " 'snt_UCS1mEytYPPiOHtfe_zqvKWg_ISuU6WFdTXM.xlsx',\n",
              " 'snt_UCS1mEytYPPiOHtfe_zqvKWg_h3xKTSrEnWI.xlsx',\n",
              " 'snt_UCS1mEytYPPiOHtfe_zqvKWg_iEP4CmYe9Pw.xlsx',\n",
              " 'snt_UCS1mEytYPPiOHtfe_zqvKWg_vpJyCM2weFM.xlsx',\n",
              " 'snt_UCS1mEytYPPiOHtfe_zqvKWg_2Qqwiw1Y60I.xlsx',\n",
              " 'snt_UCS1mEytYPPiOHtfe_zqvKWg_RDK8CrJl1ec.xlsx',\n",
              " 'snt_UCS1mEytYPPiOHtfe_zqvKWg_uiOaOtzp3KU.xlsx',\n",
              " 'snt_UCS1mEytYPPiOHtfe_zqvKWg_r5R-13xOzkw.xlsx',\n",
              " 'snt_UCS1mEytYPPiOHtfe_zqvKWg_FQkMAP4YDC8.xlsx',\n",
              " 'snt_UCS1mEytYPPiOHtfe_zqvKWg_bUFVhFFSueM.xlsx',\n",
              " 'snt_UCS1mEytYPPiOHtfe_zqvKWg_IXFCwlthCh8.xlsx',\n",
              " 'snt_UCS1mEytYPPiOHtfe_zqvKWg_y4ozVNOs0iQ.xlsx',\n",
              " 'snt_UCS1mEytYPPiOHtfe_zqvKWg_NTkPaW_HbtQ.xlsx',\n",
              " 'snt_UCS1mEytYPPiOHtfe_zqvKWg_c1uL22inFwc.xlsx',\n",
              " 'snt_UCS1mEytYPPiOHtfe_zqvKWg_-JHybfdREjM.xlsx',\n",
              " 'snt_UCS1mEytYPPiOHtfe_zqvKWg_DsSeZEO1A0E.xlsx',\n",
              " 'snt_UCS1mEytYPPiOHtfe_zqvKWg_d9nzIpmPv5s.xlsx',\n",
              " 'snt_UCS1mEytYPPiOHtfe_zqvKWg_r16_kG5hy9g.xlsx',\n",
              " 'snt_UCS1mEytYPPiOHtfe_zqvKWg_UDy1qgZ0vIs.xlsx',\n",
              " 'snt_UCS1mEytYPPiOHtfe_zqvKWg_GSGV184grGs.xlsx',\n",
              " 'snt_UCS1mEytYPPiOHtfe_zqvKWg_IFzF9jlJZfQ.xlsx',\n",
              " 'snt_UCS1mEytYPPiOHtfe_zqvKWg_t2vQ0XG-FLk.xlsx',\n",
              " 'snt_UCS1mEytYPPiOHtfe_zqvKWg_X1b6aK_A1xg.xlsx',\n",
              " 'snt_UCS1mEytYPPiOHtfe_zqvKWg_cfA7dzz6neo.xlsx',\n",
              " 'snt_UCS1mEytYPPiOHtfe_zqvKWg_P_I3asAoEms.xlsx',\n",
              " 'snt_UCS1mEytYPPiOHtfe_zqvKWg_OHtmChXHEfk.xlsx',\n",
              " 'snt_channelid_w41KD0otkUA.xlsx',\n",
              " 'lines_UCBobmJyzsJ6Ll7UbfhI4iwQ_h_dJtk3BCyg.xlsx',\n",
              " 'lines_UCBobmJyzsJ6Ll7UbfhI4iwQ_A1Nvl2MD30U.xlsx',\n",
              " 'lines_UCBobmJyzsJ6Ll7UbfhI4iwQ_alQEf454PjU.xlsx',\n",
              " 'lines_UCBobmJyzsJ6Ll7UbfhI4iwQ_r1fHOS4XaeE.xlsx',\n",
              " 'lines_UCBobmJyzsJ6Ll7UbfhI4iwQ_Wc6lUXOhRO0.xlsx',\n",
              " 'lines_UCBobmJyzsJ6Ll7UbfhI4iwQ_f9q8A-9DvPo.xlsx',\n",
              " 'lines_UCBobmJyzsJ6Ll7UbfhI4iwQ_u06GAVxyIag.xlsx',\n",
              " 'lines_UCBobmJyzsJ6Ll7UbfhI4iwQ_Fj1zCsYydD8.xlsx',\n",
              " 'lines_UCBobmJyzsJ6Ll7UbfhI4iwQ_mUCitodzZxI.xlsx',\n",
              " 'lines_UCBobmJyzsJ6Ll7UbfhI4iwQ_Dmh5a_ddO58.xlsx',\n",
              " 'lines_UCBobmJyzsJ6Ll7UbfhI4iwQ_QXuHzH0IyRE.xlsx',\n",
              " 'lines_UCBobmJyzsJ6Ll7UbfhI4iwQ_Itd677YZi50.xlsx',\n",
              " 'lines_UCBobmJyzsJ6Ll7UbfhI4iwQ_usJrcwN6T4I.xlsx',\n",
              " 'lines_UCBobmJyzsJ6Ll7UbfhI4iwQ_xLVJP-o0g28.xlsx',\n",
              " 'lines_UCBobmJyzsJ6Ll7UbfhI4iwQ_7ZLibi6s_ew.xlsx',\n",
              " '!error_19-50_snt_channelid_xLVJP-o0g28.xlsx',\n",
              " '!error_467-503_snt_channelid_xLVJP-o0g28.xlsx',\n",
              " 'lines_channelid_xLVJP-o0g28.xlsx',\n",
              " '!error_463-499_snt_channel_xLVJP-o0g28.xlsx',\n",
              " '!error_464-500_snt_channel_xLVJP-o0g28.xlsx',\n",
              " '!error_0-20_snt_channel_xLVJP-o0g28.xlsx',\n",
              " '!error_1-0_snt_UCBobmJyzsJ6Ll7UbfhI4iwQ_h_dJtk3BCyg.xlsx',\n",
              " '!error_2-0_snt_UCBobmJyzsJ6Ll7UbfhI4iwQ_A1Nvl2MD30U.xlsx',\n",
              " '!error_0-7_snt_UCBobmJyzsJ6Ll7UbfhI4iwQ_alQEf454PjU.xlsx',\n",
              " '!error_0-43_snt_UCBobmJyzsJ6Ll7UbfhI4iwQ_r1fHOS4XaeE.xlsx',\n",
              " '!error_0-6_snt_UCBobmJyzsJ6Ll7UbfhI4iwQ_Wc6lUXOhRO0.xlsx',\n",
              " '!error_0-1_snt_UCBobmJyzsJ6Ll7UbfhI4iwQ_f9q8A-9DvPo.xlsx',\n",
              " '!error_0-6_snt_UCBobmJyzsJ6Ll7UbfhI4iwQ_u06GAVxyIag.xlsx',\n",
              " '!error_0-49_snt_UCBobmJyzsJ6Ll7UbfhI4iwQ_Fj1zCsYydD8.xlsx',\n",
              " '!error_0-9_snt_UCBobmJyzsJ6Ll7UbfhI4iwQ_Dmh5a_ddO58.xlsx',\n",
              " '!error_0-18_snt_UCBobmJyzsJ6Ll7UbfhI4iwQ_QXuHzH0IyRE.xlsx',\n",
              " '!error_0-60_snt_UCBobmJyzsJ6Ll7UbfhI4iwQ_Itd677YZi50.xlsx',\n",
              " 'snt_UCBobmJyzsJ6Ll7UbfhI4iwQ_mUCitodzZxI.xlsx',\n",
              " '!error_0-24_snt_UCBobmJyzsJ6Ll7UbfhI4iwQ_usJrcwN6T4I.xlsx',\n",
              " '!error_0-20_snt_UCBobmJyzsJ6Ll7UbfhI4iwQ_xLVJP-o0g28.xlsx',\n",
              " '!error_0-17_snt_UCBobmJyzsJ6Ll7UbfhI4iwQ_7ZLibi6s_ew.xlsx',\n",
              " 'snt_UCBobmJyzsJ6Ll7UbfhI4iwQ_9qh8HPHjCAw.xlsx',\n",
              " '!error_0-3_snt_UCBobmJyzsJ6Ll7UbfhI4iwQ_WU0gvPcc3jQ.xlsx',\n",
              " 'UCBobmJyzsJ6Ll7UbfhI4iwQ']"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "# Mount my Google Drive (storage)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "# data dir\n",
        "import os\n",
        "data_dir = '/content/gdrive/MyDrive/subtitles'  # Your data directory in Colab \n",
        "os.listdir(data_dir)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# install libraries\n",
        "!pip install google-api-python-client\n",
        "!pip install youtube_transcript_api\n",
        "!pip install openai\n",
        "# importing libraries \n",
        "import json\n",
        "from googleapiclient.discovery import build # Google API request\n",
        "from youtube_transcript_api import YouTubeTranscriptApi\n",
        "import os\n",
        "import pandas as pd\n",
        "import re\n",
        "from itertools import zip_longest\n",
        "\n",
        "# Enter your YouTube api key\n",
        "api_key = 'AIzaSyA_mcs0kI1vKVFt60TuNLrqSflXk2LDhdc' \n",
        "# If the address of the Youtube is https://youtu.be/zOjov-2OZ0E then, the video id is tha last part \"zOjov-2OZ0E\"."
      ],
      "metadata": {
        "id": "sqpIl0BIA3kP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7fbc431d-8632-46a4-deb8-e910d42dd7aa"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.10/dist-packages (2.84.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client) (4.1.1)\n",
            "Requirement already satisfied: google-auth<3.0.0dev,>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client) (2.17.3)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.15.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client) (0.21.0)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client) (2.11.0)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client) (0.1.0)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-api-python-client) (3.20.3)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-api-python-client) (1.59.0)\n",
            "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-api-python-client) (2.27.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.0dev,>=1.19.0->google-api-python-client) (4.9)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.0dev,>=1.19.0->google-api-python-client) (0.3.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.0dev,>=1.19.0->google-api-python-client) (5.3.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.0dev,>=1.19.0->google-api-python-client) (1.16.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.10/dist-packages (from httplib2<1dev,>=0.15.0->google-api-python-client) (3.0.9)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0dev,>=1.19.0->google-api-python-client) (0.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-api-python-client) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-api-python-client) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-api-python-client) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-api-python-client) (2022.12.7)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting youtube_transcript_api\n",
            "  Downloading youtube_transcript_api-0.6.0-py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from youtube_transcript_api) (2.27.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->youtube_transcript_api) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->youtube_transcript_api) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->youtube_transcript_api) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->youtube_transcript_api) (3.4)\n",
            "Installing collected packages: youtube_transcript_api\n",
            "Successfully installed youtube_transcript_api-0.6.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting openai\n",
            "  Downloading openai-0.27.6-py3-none-any.whl (71 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.65.0)\n",
            "Collecting aiohttp\n",
            "  Downloading aiohttp-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai) (2.27.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.4)\n",
            "Collecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (23.1.0)\n",
            "Collecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: multidict, frozenlist, async-timeout, yarl, aiosignal, aiohttp, openai\n",
            "Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 frozenlist-1.3.3 multidict-6.0.4 openai-0.27.6 yarl-1.9.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get all videos ID from channel"
      ],
      "metadata": {
        "id": "oocvW6hmQuoG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "channelId = \"UCBobmJyzsJ6Ll7UbfhI4iwQ\"\n",
        "youtube = build('youtube','v3',developerKey= api_key)\n",
        "\n",
        "# getting all video details\n",
        "contentdata = youtube.channels().list(id=channelId,part='contentDetails').execute()\n",
        "playlist_id = contentdata['items'][0]['contentDetails']['relatedPlaylists']['uploads']\n",
        "videos = []\n",
        "next_page_token = None\n",
        "\n",
        "while 1:\n",
        "    res = youtube.playlistItems().list(playlistId=playlist_id,part='snippet',maxResults=50,pageToken=next_page_token).execute()\n",
        "    videos += res['items']\n",
        "    next_page_token = res.get('nextPageToken')\n",
        "    if next_page_token is None:\n",
        "        break\n",
        "\n",
        "# getting video id for each video\n",
        "video_ids = list(map(lambda x:x['snippet']['resourceId']['videoId'], videos))\n",
        "# video_ids = video_ids[:4]\n",
        "video_ids"
      ],
      "metadata": {
        "id": "AZ2iIC4OGnmJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Filter videos ids by Language and save to GD"
      ],
      "metadata": {
        "id": "BNrdhDGOt4yS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a new directory if it doesn't exist\n",
        "def checkLanguages(video_id):\n",
        "    print(f'Checking {video_id}')\n",
        "    try:\n",
        "        subtitles_en = YouTubeTranscriptApi.get_transcript(video_id, languages=['en'])\n",
        "        subtitles_ru = YouTubeTranscriptApi.get_transcript(video_id, languages=['ru'])\n",
        "    except:\n",
        "        return  False\n",
        "    print(f'Languages find in {video_id}')\n",
        "    return True       \n",
        "\n",
        "video_ids = [el for el in video_ids if checkLanguages(el)]\n",
        "\n",
        "directory = os.path.join(data_dir, channelId)\n",
        "\n",
        "if not os.path.exists(directory):\n",
        "    os.makedirs(directory)\n",
        "\n",
        "file_path = os.path.join(directory, 'video_list.xlsx')\n",
        "df = pd.DataFrame(video_ids, columns=['video_ids'])\n",
        "df.to_excel(file_path, index=False)\n",
        "\n",
        "print(f\"List saved as {file_path}\")"
      ],
      "metadata": {
        "id": "mJS_IdSQuIaT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load videos ID from GD"
      ],
      "metadata": {
        "id": "VeEMGm6IuTXl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "file_path = \"/content/gdrive/MyDrive/subtitles/UCBobmJyzsJ6Ll7UbfhI4iwQ/video_list.xlsx\"\n",
        "channelId = \"Epic\"\n",
        "\n",
        "# Read the Excel file into a pandas DataFrame\n",
        "df = pd.read_excel(file_path)\n",
        "\n",
        "# Convert the DataFrame column to a list\n",
        "video_ids = df['video_ids'].tolist()\n",
        "\n",
        "print(video_ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2KEB1t5DuZjv",
        "outputId": "7d814ca0-7a08-432e-c2db-b0fbe7c1ee8c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['h_dJtk3BCyg', 'A1Nvl2MD30U', 'alQEf454PjU', 'r1fHOS4XaeE', 'Wc6lUXOhRO0', 'f9q8A-9DvPo', 'u06GAVxyIag', 'Fj1zCsYydD8', 'mUCitodzZxI', 'Dmh5a_ddO58', 'QXuHzH0IyRE', 'Itd677YZi50', 'usJrcwN6T4I', 'xLVJP-o0g28', '7ZLibi6s_ew', '9qh8HPHjCAw', 'WU0gvPcc3jQ']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get subtitles by lines"
      ],
      "metadata": {
        "id": "JJMSbG31uUsf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# list to hold all prompts\n",
        "prompts = []\n",
        "\n",
        "# iterate over each video ID\n",
        "for video_id in video_ids:\n",
        "\n",
        "    try:\n",
        "        subtitles_en = YouTubeTranscriptApi.get_transcript(video_id, languages=['en'])\n",
        "        subtitles_ru = YouTubeTranscriptApi.get_transcript(video_id, languages=['ru'])\n",
        "    except Exception as e:\n",
        "        print(f\"Exception occurred: {e}\")\n",
        "        continue\n",
        "    list_sub_en = []\n",
        "    list_sub_ru = []\n",
        "    # iterate over each subtitle entry and add to prompts list\n",
        "    for sub_en, sub_ru in zip(subtitles_en, subtitles_ru):\n",
        "        text_en = sub_en['text']\n",
        "        text_ru = sub_ru['text']\n",
        "        list_sub_en.append(text_en)\n",
        "        list_sub_ru.append(text_ru)\n",
        "    max_len = max(len(list_sub_en), len(list_sub_en))\n",
        "    data = list(zip_longest(list_sub_en, list_sub_en, fillvalue=None))\n",
        "    df = pd.DataFrame(data, columns=['en', 'ru'])\n",
        "    sufix = \"\"\n",
        "    if len(list_sub_en) != len(list_sub_ru): sufix = f\"!error_{len(list_sub_en)}-{len(list_sub_ru)}_\"\n",
        "    with pd.ExcelWriter(os.path.join(data_dir, f'{sufix}lines_{channelId}_{video_id}.xlsx')) as writer:\n",
        "        df.to_excel(writer, index=False)\n",
        "    \n",
        "#sentences = combined_sub.split('\\n')\n",
        "#combined_subtitles.extend(sentences)\n",
        "\n",
        "\n",
        "# create dataframe from prompts list\n",
        "#df = pd.DataFrame(prompts, columns=['en', 'ru'])\n",
        "\n",
        "# save dataframe to Excel file\n",
        "#with pd.ExcelWriter(os.path.join(data_dir, f'{channelId}.xlsx')) as writer:\n",
        "#    df.to_excel(writer, index=False)"
      ],
      "metadata": {
        "id": "pAD_vIbHK6IN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get subtitles group by sentences with timecodes "
      ],
      "metadata": {
        "id": "AKRGrFbzuZO4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import os\n",
        "from itertools import zip_longest\n",
        "\n",
        "def extract_numbers(s):\n",
        "    pattern = r'\\[(\\d+\\.\\d+)\\]'\n",
        "    numbers = re.findall(pattern, s)\n",
        "    cleaned_s = re.sub(pattern, '', s)\n",
        "    return numbers, cleaned_s\n",
        "\n",
        "def compareLists(list1, list2):\n",
        "  i = 0\n",
        "  while i < min(len(list1), len(list2)):\n",
        "    numbers1_1, cleaned_s1_1 = extract_numbers(list1[i])\n",
        "    numbers2_1, cleaned_s2_1 = extract_numbers(list2[i])\n",
        "    if numbers1_1 == numbers2_1:\n",
        "        list1[i] = cleaned_s1_1\n",
        "        list2[i] = cleaned_s2_1\n",
        "        i += 1\n",
        "        continue\n",
        "        \n",
        "    if i < min(len(list1), len(list2)) :\n",
        "        try:\n",
        "          numbers1_2, cleaned_s1_2 = extract_numbers(list1[i+1])\n",
        "        except:\n",
        "          numbers1_2 = \"\"\n",
        "        try:\n",
        "          numbers2_2, cleaned_s2_2 = extract_numbers(list2[i+1])\n",
        "        except:\n",
        "          numbers2_2 = \"\"\n",
        "        if numbers1_1 == numbers2_2:\n",
        "            del list2[i]\n",
        "        elif numbers2_1 == numbers1_2:\n",
        "            del list1[i]\n",
        "        else:\n",
        "            del list1[i]\n",
        "            del list2[i]\n",
        "  return list1, list2\n",
        "\n",
        "# list to hold all prompts\n",
        "prompts = []\n",
        "\n",
        "# iterate over each video ID\n",
        "for video_id in video_ids:\n",
        "    print(video_id)\n",
        "\n",
        "    try:\n",
        "        subtitles_en = YouTubeTranscriptApi.get_transcript(video_id, languages=['en'])\n",
        "        subtitles_ru = YouTubeTranscriptApi.get_transcript(video_id, languages=['ru'])\n",
        "    except:\n",
        "        # skip video if it doesn't have English or Russian subtitles\n",
        "        continue\n",
        "    combined_sub_en = \"\"\n",
        "    combined_sub_ru = \"\"\n",
        "    # iterate over each subtitle entry and add to prompts list\n",
        "    for sub_en, sub_ru in zip(subtitles_en, subtitles_ru):\n",
        "        text_en = sub_en['text']\n",
        "        time_en = sub_en['start']\n",
        "\n",
        "        text_ru = sub_ru['text']\n",
        "        time_ru = sub_ru['start']\n",
        "\n",
        "        text_en = re.split(r'(?<=[!?.])+(?=[A-ZА-Я])', text_en)\n",
        "        text_en = \" \".join(text_en)\n",
        "        text_ru = re.split(r'(?<=[!?.])+(?=[A-ZА-Я])', text_ru)\n",
        "        text_ru = \" \".join(text_ru)\n",
        "        text_en = re.sub(r'\\{\\an\\d+\\}\\s*', '', text_en)\n",
        "        text_ru = re.sub(r'\\{\\an\\d+\\}\\s*', '', text_ru)\n",
        "        text_en = re.sub(r'\\[.*?\\]', '', text_en)\n",
        "        text_ru = re.sub(r'\\[.*?\\]', '', text_ru)\n",
        "        text_en = re.sub(r'[A-ZА-Я ]+:', '', text_en)\n",
        "        text_ru = re.sub(r'[A-ZА-Я ]+:', '', text_ru)\n",
        "\n",
        "        combined_sub_en += (text_en.replace('\\xa0', ' ').replace('\\n', ' ').replace('...', ',').replace('…', ',').replace('\"', '').replace('\\'', '') + ' ').replace('  ', ' ').replace('  ', ' ') + f\"[{time_en}]\"\n",
        "        combined_sub_ru += (text_ru.replace('\\xa0', ' ').replace('\\n', ' ').replace('...', ',').replace('…', ',').replace('«', '').replace('»', '').replace('\"', '') + ' ').replace('  ', ' ').replace('  ', ' ') + f\"[{time_ru}]\"\n",
        "\n",
        "#        sentences_sub_en = re.split(r'(?<![!?\\.])[!?.]\\s', combined_sub_en)\n",
        "#        sentences_sub_ru = re.split(r'(?<![!?\\.])[!?.]\\s', combined_sub_ru)\n",
        "\n",
        "    sentences_sub_en = re.split(r'(?<=[!?.])\\s+(?=[A-ZА-Я])', combined_sub_en)\n",
        "    sentences_sub_ru = re.split(r'(?<=[!?.])\\s+(?=[A-ZА-Я])', combined_sub_ru)\n",
        "    \n",
        "#    sentences_sub_en, sentences_sub_ru = compareLists(sentences_sub_en, sentences_sub_ru)\n",
        "\n",
        "\n",
        "    sentences_sub_en = [elem for elem in sentences_sub_en if elem]\n",
        "    sentences_sub_ru = [elem for elem in sentences_sub_ru if elem]\n",
        "\n",
        "    print(video_id)\n",
        "    print(len(sentences_sub_en))\n",
        "    print(len(sentences_sub_ru))\n",
        "    print(\"------------------\")\n",
        "\n",
        "    max_len = max(len(sentences_sub_en), len(sentences_sub_ru))\n",
        "    data = list(zip_longest(sentences_sub_en, sentences_sub_ru, fillvalue=None))\n",
        "    df = pd.DataFrame(data, columns=['en', 'ru'])\n",
        "    sufix = \"\"\n",
        "    if len(sentences_sub_en) != len(sentences_sub_ru): sufix = f\"!error_{len(sentences_sub_en)}-{len(sentences_sub_ru)}_\"\n",
        "    with pd.ExcelWriter(os.path.join(data_dir, f'{sufix}snt_{channelId}_{video_id}.xlsx')) as writer:\n",
        "        df.to_excel(writer, index=False)\n",
        "    \n",
        "#sentences = combined_sub.split('\\n')\n",
        "#combined_subtitles.extend(sentences)\n",
        "\n",
        "\n",
        "# create dataframe from prompts list\n",
        "#df = pd.DataFrame(prompts, columns=['en', 'ru'])\n",
        "\n",
        "# save dataframe to Excel file\n",
        "#with pd.ExcelWriter(os.path.join(data_dir, f'{channelId}.xlsx')) as writer:\n",
        "#    df.to_excel(writer, index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 322
        },
        "id": "0p76M8MJPnkg",
        "outputId": "368815eb-dfae-40eb-e92b-ed1f578f9064"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "h_dJtk3BCyg\n",
            "h_dJtk3BCyg\n",
            "2\n",
            "1\n",
            "------------------\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-e8f540db6d87>\u001b[0m in \u001b[0;36m<cell line: 45>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0msufix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences_sub_en\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences_sub_ru\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msufix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"!error_{len(sentences_sub_en)}-{len(sentences_sub_ru)}_\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExcelWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf'{sufix}snt_{channelId}_{video_id}.xlsx'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mwriter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m         \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwriter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'channelId' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get subtitles group by sentences with embeddings "
      ],
      "metadata": {
        "id": "m89F4_Qv9mT0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import os\n",
        "import openai\n",
        "from scipy.spatial.distance import cosine\n",
        "from openai.embeddings_utils import get_embedding\n",
        "from itertools import zip_longest\n",
        "\n",
        "\n",
        "def get_text_similarity(text1, text2):\n",
        "    openai.api_key = \"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\"  # Replace with your own OpenAI API key\n",
        "    model_engine = \"text-embedding-ada-002\"  # Change to the desired OpenAI language model\n",
        "\n",
        "    # Generate embeddings for the two pieces of text using OpenAI's language model\n",
        "    embeddings1 = get_embedding(text1, model_engine)\n",
        "    embeddings2 = get_embedding(text2, model_engine)\n",
        "\n",
        "    # Calculate the cosine distance between the two embeddings using Scipy's cosine distance function\n",
        "    similarity = 1 - cosine(embeddings1, embeddings2)\n",
        "\n",
        "    return similarity\n",
        "\n",
        "def compareLists(list1, list2):\n",
        "  i = 0\n",
        "  while i < min(len(list1), len(list2)):\n",
        "    print(i)\n",
        "    similarity = get_text_similarity(list1[i], list2[i])\n",
        "    if similarity > 0.81:\n",
        "        i += 1\n",
        "        continue\n",
        "    \n",
        "    if i < min(len(list1), len(list2)) :\n",
        "        try:\n",
        "          similarity1 = get_text_similarity(list1[i], list2[i+1])\n",
        "        except:\n",
        "          similarity1 = 0\n",
        "        try:\n",
        "          similarity2 = get_text_similarity(list1[i+1], list2[i])\n",
        "        except:\n",
        "          similarity2 = 0\n",
        "        if similarity1 > 0.81:\n",
        "            del list2[i]\n",
        "        elif similarity2 > 0.81:\n",
        "            del list1[i]\n",
        "        else:\n",
        "            del list1[i]\n",
        "            del list2[i]\n",
        "  return list1, list2\n",
        "\n",
        "# list to hold all prompts\n",
        "prompts = []\n",
        "video_ids = [\"WU0gvPcc3jQ\"]\n",
        "channelId = \"channel\"\n",
        "\n",
        "\n",
        "# iterate over each video ID\n",
        "for video_id in video_ids:\n",
        "    print(video_id)\n",
        "\n",
        "    try:\n",
        "        subtitles_en = YouTubeTranscriptApi.get_transcript(video_id, languages=['en'])\n",
        "        subtitles_ru = YouTubeTranscriptApi.get_transcript(video_id, languages=['ru'])\n",
        "    except:\n",
        "        # skip video if it doesn't have English or Russian subtitles\n",
        "        continue\n",
        "    combined_sub_en = \"\"\n",
        "    combined_sub_ru = \"\"\n",
        "    # iterate over each subtitle entry and add to prompts list\n",
        "    for sub_en, sub_ru in zip(subtitles_en, subtitles_ru):\n",
        "        text_en = sub_en['text']\n",
        "        time_en = sub_en['start']\n",
        "\n",
        "        text_ru = sub_ru['text']\n",
        "        time_ru = sub_ru['start']\n",
        "\n",
        "        text_en = re.split(r'(?<=[!?.])+(?=[A-ZА-Я])', text_en)\n",
        "        text_en = \" \".join(text_en)\n",
        "        text_ru = re.split(r'(?<=[!?.])+(?=[A-ZА-Я])', text_ru)\n",
        "        text_ru = \" \".join(text_ru)\n",
        "        text_en = re.sub(r'\\{\\an\\d+\\}\\s*', '', text_en)\n",
        "        text_ru = re.sub(r'\\{\\an\\d+\\}\\s*', '', text_ru)\n",
        "        text_en = re.sub(r'\\[.*?\\]', '', text_en)\n",
        "        text_ru = re.sub(r'\\[.*?\\]', '', text_ru)\n",
        "        text_en = re.sub(r'[A-ZА-Я ]+:', '', text_en)\n",
        "        text_ru = re.sub(r'[A-ZА-Я ]+:', '', text_ru)\n",
        "        text_en = text_en.replace('Hi!', 'Hi,')\n",
        "        text_ru = text_ru.replace('Привет!', 'Привет,')\n",
        "        text_en = text_en.replace('Hi.', 'Hi,')\n",
        "        text_ru = text_ru.replace('Привет.', 'Привет,')\n",
        "        combined_sub_en += (text_en.replace('\\xa0', ' ').replace('\\n', ' ').replace('...', ',').replace('…', ',').replace('\"', '').replace('\\'', '') + ' ').replace('  ', ' ').replace('  ', ' ')\n",
        "        combined_sub_ru += (text_ru.replace('\\xa0', ' ').replace('\\n', ' ').replace('...', ',').replace('…', ',').replace('«', '').replace('»', '').replace('\"', '') + ' ').replace('  ', ' ').replace('  ', ' ')\n",
        "\n",
        "#        sentences_sub_en = re.split(r'(?<![!?\\.])[!?.]\\s', combined_sub_en)\n",
        "#        sentences_sub_ru = re.split(r'(?<![!?\\.])[!?.]\\s', combined_sub_ru)\n",
        "\n",
        "    sentences_sub_en = re.split(r'(?<=[!?.])\\s+(?=[A-ZА-Я])', combined_sub_en)\n",
        "    sentences_sub_ru = re.split(r'(?<=[!?.])\\s+(?=[A-ZА-Я])', combined_sub_ru)\n",
        "    \n",
        "    sentences_sub_en, sentences_sub_ru = compareLists(sentences_sub_en, sentences_sub_ru)\n",
        "\n",
        "    sentences_sub_en = [elem for elem in sentences_sub_en if elem]\n",
        "    sentences_sub_ru = [elem for elem in sentences_sub_ru if elem]\n",
        "\n",
        "    print(video_id)\n",
        "    print(len(sentences_sub_en))\n",
        "    print(len(sentences_sub_ru))\n",
        "    print(\"------------------\")\n",
        "\n",
        "    max_len = max(len(sentences_sub_en), len(sentences_sub_ru))\n",
        "    data = list(zip_longest(sentences_sub_en, sentences_sub_ru, fillvalue=None))\n",
        "    df = pd.DataFrame(data, columns=['en', 'ru'])\n",
        "    sufix = \"\"\n",
        "    if len(sentences_sub_en) != len(sentences_sub_ru): sufix = f\"!error_{len(sentences_sub_en)}-{len(sentences_sub_ru)}_\"\n",
        "    with pd.ExcelWriter(os.path.join(data_dir, f'{sufix}snt_{channelId}_{video_id}.xlsx')) as writer:\n",
        "        df.to_excel(writer, index=False)\n",
        "    \n",
        "#sentences = combined_sub.split('\\n')\n",
        "#combined_subtitles.extend(sentences)\n",
        "\n",
        "\n",
        "# create dataframe from prompts list\n",
        "#df = pd.DataFrame(prompts, columns=['en', 'ru'])\n",
        "\n",
        "# save dataframe to Excel file\n",
        "#with pd.ExcelWriter(os.path.join(data_dir, f'{channelId}.xlsx')) as writer:\n",
        "#    df.to_excel(writer, index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6abda945-97ac-457c-b1db-6e6412feb546",
        "id": "5dMP_BIB9lD8"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WU0gvPcc3jQ\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "25\n",
            "25\n",
            "26\n",
            "27\n",
            "28\n",
            "29\n",
            "30\n",
            "31\n",
            "32\n",
            "33\n",
            "34\n",
            "35\n",
            "35\n",
            "36\n",
            "37\n",
            "38\n",
            "39\n",
            "39\n",
            "40\n",
            "40\n",
            "41\n",
            "41\n",
            "42\n",
            "42\n",
            "43\n",
            "43\n",
            "44\n",
            "45\n",
            "46\n",
            "47\n",
            "48\n",
            "49\n",
            "50\n",
            "51\n",
            "52\n",
            "53\n",
            "54\n",
            "55\n",
            "55\n",
            "56\n",
            "57\n",
            "58\n",
            "58\n",
            "59\n",
            "59\n",
            "60\n",
            "61\n",
            "61\n",
            "61\n",
            "62\n",
            "WU0gvPcc3jQ\n",
            "62\n",
            "63\n",
            "------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Further test cells, they are not needed"
      ],
      "metadata": {
        "id": "ROAw3Jvfb9ha"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "list1 = ['apple', 'banana [1.1]', 'cherry[2.1]',  'banana1', 'banana2',       'apple1[1.23]',  'banana1[12.2]', 'banana5[1.22]']\n",
        "list2 = ['apple', 'banana [1.1]', 'cherry[2.2]',             'banana3',       'apple2[1.23]',  'banana2[12.3]', 'banana2', 'banana6[1.22]']\n",
        "\n",
        "def compareLists(list1, list2):\n",
        "  i = 0\n",
        "  while i < min(len(list1), len(list2)):\n",
        "    numbers1_1, cleaned_s1_1 = extract_numbers(list1[i])\n",
        "    numbers2_1, cleaned_s2_1 = extract_numbers(list2[i])\n",
        "    print(i)\n",
        "    if numbers1_1 == numbers2_1:\n",
        "        print(f\"{list1[i] = } {list2[i] = }\")\n",
        "        list1[i] = cleaned_s1_1\n",
        "        list2[i] = cleaned_s2_1\n",
        "        i += 1\n",
        "        continue\n",
        "        \n",
        "    del list1[i]\n",
        "    del list2[i]\n",
        "    if i < min(len(list1), len(list2)) :\n",
        "        numbers1_1, cleaned_s1_1 = extract_numbers(list1[i])\n",
        "        numbers2_1, cleaned_s2_1 = extract_numbers(list2[i])\n",
        "        try:\n",
        "          numbers1_2, cleaned_s1_2 = extract_numbers(list1[i+1])\n",
        "        except:\n",
        "          numbers1_2 = \"\"\n",
        "        try:\n",
        "          numbers2_2, cleaned_s2_2 = extract_numbers(list2[i+1])\n",
        "        except:\n",
        "          numbers2_2 = \"\"\n",
        "        if numbers1_1 == numbers2_2:\n",
        "            del list2[i]\n",
        "        elif numbers2_1 == numbers1_2:\n",
        "            del list1[i]\n",
        "  return list1, list2\n",
        "        \n",
        "list1, list2 = compareLists(list1, list2)\n",
        "\n",
        "print(list1)\n",
        "print(list2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iXOnwHyYFhiQ",
        "outputId": "a6d1e74f-a286-412a-8f73-5f4605247fb8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "list1[i] = 'apple' list2[i] = 'apple'\n",
            "1\n",
            "list1[i] = 'banana [1.1]' list2[i] = 'banana [1.1]'\n",
            "2\n",
            "2\n",
            "list1[i] = 'banana2' list2[i] = 'banana3'\n",
            "3\n",
            "list1[i] = 'apple1[1.23]' list2[i] = 'apple2[1.23]'\n",
            "4\n",
            "4\n",
            "list1[i] = 'banana5[1.22]' list2[i] = 'banana6[1.22]'\n",
            "['apple', 'banana ', 'banana2', 'apple1', 'banana5']\n",
            "['apple', 'banana ', 'banana3', 'apple2', 'banana6']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YICHgkJQ37HM",
        "outputId": "fafd202e-6fe1-46fc-da56-681117040c8f"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7760193696146548\n"
          ]
        }
      ]
    }
  ]
}